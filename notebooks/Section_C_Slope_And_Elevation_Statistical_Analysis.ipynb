{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mickymags/curriculum_development_initiative/blob/main/notebooks/Section_C_Slope_And_Elevation_Statistical_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTItAdgo_sqU"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "In this script, we will conduct a statistical analysis of slope and elevation across all SERVIR countries. Our end goal is to obtain quartile values for both slope elevation. That is, what are the three elevation/slope values that will separate the number of pixels into four equal groups?\n",
        "\n",
        "In Part 1, we will get extract geometries for our SERVIR regions. We will then split the geometry covering SERVIR regions at large into smaller geometries, as attempting to export this data for a large region results in computational errors from GEE. For exceptionally large countries such as Brazil & Mali, we will have to split these countries into smaller areas.\n",
        "\n",
        "In Part 2, we will obtain the Shuttle Radar Topography Mission (SRTM) elevation data from Google Earth Engine and use the `ee.Algorithms.Terrain()` method to calculate the slope based on that elevation data. Then, for each of our subregions we created in Part 1, we will export the slope and elevation information in tabular format.\n",
        "\n",
        "In Part 3, we will combine the data from our subregions into three arrays, one representing the pixel values (either for slope or elevation), one representing the number of pixels with that corresponding elevation value, and the final represeting the number of pixels with that corresponding slope value.\n",
        "\n",
        "In Part 4, we will plot the elevation and slope values to find good quartile values (i.e. 0.25 quartile, 0.5 quartile (median), and 0.75 quartile) for both slope and elevation.\n",
        "\n",
        "**NOTICE: This script will take about 1-2 hours to run and may return computational timeouts depending on GEE compute availability.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT5rLt8BFwdU"
      },
      "source": [
        "# Part 1: Get Region(s) of Interest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-pqJRYNGI3L"
      },
      "source": [
        "### Part 1 Step 1: Import Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mh1c373GMZ1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import geemap\n",
        "import ee"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDbJnT3iGrm4"
      },
      "source": [
        "In order to continue running this script, you need to be associated with a Google Cloud Project. Now, we have to authenticate and initialize earth engine. After you run the code below, click through the pop-up window to login to the Google Account associated with your Google Earth Engine account. Click \"Continue\" until you have returned to this notebook and a green checkmark appears to the left of the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm5hf5ASGuUI"
      },
      "outputs": [],
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize(project = 'servir-sco-assets')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlLYEBI8G7sN"
      },
      "source": [
        "## Part 1 Step 2: Get SERVIR countries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTdO00b3IvuE"
      },
      "source": [
        "Get the Global Administrative Unit Layers (GAUL) dataset produced by the United Nations Food And Agriculture Organization. Then extract each SERVIR country using the `.filter` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z16EHXRHHYBO"
      },
      "outputs": [],
      "source": [
        "# gaul = Global Administrative Unit Layers\n",
        "gaul = ee.FeatureCollection(\"FAO/GAUL_SIMPLIFIED_500m/2015/level0\")\n",
        "\n",
        "##Amazonia\n",
        "br = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Brazil'))\n",
        "co = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Colombia'))\n",
        "ec = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Ecuador'))\n",
        "pe = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Peru'))\n",
        "gu = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Guyana'))\n",
        "su = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Suriname'))\n",
        "ba = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Bahamas'))\n",
        "bb = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Barbados'))\n",
        "dr = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Dominican Republic'))\n",
        "tt = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Trinidad and Tobago'))\n",
        "\n",
        "#Central America\n",
        "bz = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Belize'))\n",
        "cr = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Costa Rica'))\n",
        "es= gaul.filter(ee.Filter.eq('ADM0_NAME', 'El Salvador'))\n",
        "gt = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Guatemala'))\n",
        "ho = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Honduras'))\n",
        "pa = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Panama'))\n",
        "\n",
        "#HKH\n",
        "bg = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Bangladesh'))\n",
        "ne = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Nepal'))\n",
        "pk = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Pakistan'))\n",
        "bt = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Bhutan'))\n",
        "\n",
        "#Southeast Asia\n",
        "ca = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Cambodia'))\n",
        "id = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Indonesia'))\n",
        "vn = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Viet Nam'))\n",
        "lo = gaul.filter(ee.Filter.eq('ADM0_NAME', \"Lao People's Democratic Republic\"))\n",
        "my = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Myanmar'))\n",
        "ph = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Philippines'))\n",
        "th = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Thailand'))\n",
        "\n",
        "#West Africa\n",
        "se = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Senegal'))\n",
        "mi = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Mali'))\n",
        "bf = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Burkina Faso'))\n",
        "gh = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Ghana'))\n",
        "ni = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Niger'))\n",
        "na = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Nigeria'))\n",
        "mo = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Morocco'))\n",
        "al = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Algeria'))\n",
        "ma = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Mauritania'))\n",
        "gb = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Guinea-Bissau'))\n",
        "ga = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Guinea'))\n",
        "sl = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Sierra Leone'))\n",
        "la = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Liberia'))\n",
        "cd = gaul.filter(ee.Filter.eq('ADM0_NAME', \"CÃ´te d'Ivoire\"))\n",
        "to = gaul.filter(ee.Filter.eq('ADM0_NAME', \"Togo\"))\n",
        "bn = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Benin'))\n",
        "ch = gaul.filter(ee.Filter.eq('ADM0_NAME', 'Chad'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75Rz_J8jqEQZ"
      },
      "outputs": [],
      "source": [
        "mag = ma.geometry().getInfo()\n",
        "bng = bn.geometry().getInfo()\n",
        "#print(mag)\n",
        "print(bn.getInfo())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLojnC1RJmd5"
      },
      "outputs": [],
      "source": [
        "# Create different SERVIR West Africa Subregions\n",
        "swa1 = se.merge(bf) # SERVIR West Affrica Region 1\n",
        "swa2 = gh.merge(ni) # \"                        \" 2\n",
        "swa3 = na\n",
        "swa4 = al\n",
        "swa5 = ga.merge(gb)\n",
        "swa6 = sl.merge(la)\n",
        "swa7 = cd.merge(to)\n",
        "swa8 = mi         # Mali not working\n",
        "swa9 = ch\n",
        "swa10 = ma\n",
        "swa11 = mo\n",
        "swa12 = bn\n",
        "\n",
        "swa = swa1.merge(swa2).merge(swa3).merge(swa4).merge(swa5).merge(swa6).merge(swa7).merge(swa8).merge(swa9).merge(swa10).merge(swa11).merge(swa12)\n",
        "\n",
        "# Create Different Regions for SERVIR Hindu Kush Himalaya\n",
        "shkh1 = bg;\n",
        "shkh2 = ne;\n",
        "shkh3 = pk\n",
        "shkh4 = bt;\n",
        "\n",
        "shkh = shkh1.merge(shkh2).merge(shkh3).merge(shkh4)\n",
        "\n",
        "# Create Different Regions for SERVIR Amazonia\n",
        "\n",
        "samz1 = co\n",
        "samz2 = ec;\n",
        "samz3 = pe;\n",
        "samz4 = gu.merge(su);\n",
        "samz5 = ba                # Bahamas not working\n",
        "\n",
        "\n",
        "# Brazil is too large to run this code alone, so we will need to split it in half\n",
        "gaul2 = ee.FeatureCollection(\"FAO/GAUL_SIMPLIFIED_500m/2015/level1\")\n",
        "\n",
        "brzl1 = gaul2.filter(ee.Filter.eq('ADM0_NAME', 'Brazil'))\n",
        "acre = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Acre'))\n",
        "alagoas = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Alagoas'))\n",
        "amapa = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Amapa'))\n",
        "amazonas = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Amazonas'))\n",
        "bahia = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Bahia'))\n",
        "ceara = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Ceara'))\n",
        "df = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Distrito Federal'))\n",
        "espirito_santo = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Espirito Santo'))\n",
        "goias = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Goias'))\n",
        "maranhao = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Maranhao'))\n",
        "matogrosso = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Mato Grosso'))\n",
        "mgds = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Mato Grosso Do Sul')) #check\n",
        "minasgerais = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Minas Gerais')) #check\n",
        "para = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Para')) #check\n",
        "paraiba = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Paraiba')) #check\n",
        "parana = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Parana')) #check\n",
        "pernambuco = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Pernambuco')) #check\n",
        "piaui = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Piaui')) #check\n",
        "rdj = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Rio De Janeiro')) #check\n",
        "rgdn = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Rio Grande Do Norte')) #check\n",
        "rgds = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Rio Grande Do Sul')) #check\n",
        "rondonia = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Rondonia')) #check\n",
        "roraima = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Roraima')) #check\n",
        "santa_catarina = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Santa Catarina')) #check\n",
        "sao_paulo = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Sao Paulo')) #check\n",
        "sergipe = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Sergipe')) #check\n",
        "tocantins = brzl1.filter(ee.Filter.eq('ADM1_NAME', 'Tocantins')) #check\n",
        "\n",
        "samz6 = acre.merge(alagoas)\n",
        "\n",
        "samz7 = paraiba.merge(parana)\n",
        "\n",
        "samz8 = santa_catarina.merge(sao_paulo)\n",
        "\n",
        "samz9 = ceara.merge(df).merge(espirito_santo).merge(goias)\n",
        "\n",
        "samz10 = rdj\n",
        "\n",
        "samz11 = maranhao\n",
        "\n",
        "samz12 = amapa.merge(amazonas)\n",
        "\n",
        "samz13 = rondonia.merge(roraima)\n",
        "\n",
        "samz14 = minasgerais\n",
        "\n",
        "samz15 = pernambuco.merge(piaui)\n",
        "\n",
        "samz16 = dr\n",
        "\n",
        "samz17 = bb\n",
        "\n",
        "samz18 = para\n",
        "\n",
        "samz19 = rgdn\n",
        "\n",
        "samz20 = matogrosso.merge(bahia)\n",
        "\n",
        "samz21 = rgds\n",
        "\n",
        "samz22 = mgds\n",
        "\n",
        "samz23 = sergipe.merge(tocantins)\n",
        "\n",
        "samz = samz1.merge(samz2).merge(samz3).merge(samz4).merge(samz5).merge(samz6) \\\n",
        "       .merge(samz7).merge(samz8).merge(samz9).merge(samz10).merge(samz11)    \\\n",
        "       .merge(samz12).merge(samz13).merge(samz14).merge(samz15).merge(samz16) \\\n",
        "       .merge(samz17).merge(samz18).merge(samz19).merge(samz20).merge(samz21) \\\n",
        "       .merge(samz22).merge(samz23)\n",
        "\n",
        "# Create Different Regions for SERVIR Southeast Asia\n",
        "sea1 = ca\n",
        "sea2 = vn.merge(lo);\n",
        "sea3 = ph;\n",
        "sea4 = th;\n",
        "sea5 = my;\n",
        "\n",
        "#Indonesia needs to be split into multiple regions\n",
        "id2 = gaul2.filter(ee.Filter.eq('ADM0_NAME', 'Indonesia'));\n",
        "\n",
        "maluku = id2.filter(ee.Filter.eq('ADM1_NAME', 'Maluku'));\n",
        "maluku_utara = id2.filter(ee.Filter.eq('ADM1_NAME', 'Maluku Utara'))\n",
        "bangka_belitung = id2.filter(ee.Filter.eq('ADM1_NAME', 'Bangka Belitung'))\n",
        "banten = id2.filter(ee.Filter.eq('ADM1_NAME', 'Banten'))\n",
        "gorontalo = id2.filter(ee.Filter.eq('ADM1_NAME', 'Gorontalo'))\n",
        "jawa_barat = id2.filter(ee.Filter.eq('ADM1_NAME', 'Jawa Barat'))\n",
        "sulawesi_utara = id2.filter(ee.Filter.eq('ADM1_NAME', 'Sulawesi Utara'))\n",
        "sumatera_selatan = id2.filter(ee.Filter.eq('ADM1_NAME', 'Sumatera Selatan'))\n",
        "kepulauan_riau = id2.filter(ee.Filter.eq('ADM1_NAME', 'Kepulauan-riau'))\n",
        "riau = id2.filter(ee.Filter.eq('ADM1_NAME', 'Riau'))\n",
        "sulawesi_barat = id2.filter(ee.Filter.eq('ADM1_NAME', 'Sulawesi Barat'))\n",
        "sulawesi_selatan = id2.filter(ee.Filter.eq('ADM1_NAME', 'Sulawesi Selatan'))\n",
        "nad = id2.filter(ee.Filter.eq('ADM1_NAME', 'Nangroe Aceh Darussalam'))\n",
        "bali = id2.filter(ee.Filter.eq('ADM1_NAME', 'Bali'))\n",
        "bengkulu = id2.filter(ee.Filter.eq('ADM1_NAME', 'Bengkulu'))\n",
        "day = id2.filter(ee.Filter.eq('ADM1_NAME', 'Daerah Istimewa Yogyakarta'))\n",
        "dki_jakarta = id2.filter(ee.Filter.eq('ADM1_NAME', 'Dki Jakarta'))\n",
        "jambi = id2.filter(ee.Filter.eq('ADM1_NAME', 'Jambi'))\n",
        "jawa_tengah = id2.filter(ee.Filter.eq('ADM1_NAME', 'Jawa Tengah'))\n",
        "jawa_timur = id2.filter(ee.Filter.eq('ADM1_NAME', 'Jawa Timur'))\n",
        "kali_barat = id2.filter(ee.Filter.eq('ADM1_NAME', 'Kalimantan Barat'));\n",
        "kali_sela = id2.filter(ee.Filter.eq('ADM1_NAME', 'Kalimantan Selatan'));\n",
        "kali_teng = id2.filter(ee.Filter.eq('ADM1_NAME', 'Kalimantan Tengah'));\n",
        "kali_timur = id2.filter(ee.Filter.eq('ADM1_NAME', 'Kalimantan Timur'))\n",
        "lampung = id2.filter(ee.Filter.eq('ADM1_NAME', 'Lampung'))\n",
        "nusa_barat = id2.filter(ee.Filter.eq('ADM1_NAME', 'Nusatenggara Barat'))\n",
        "nusa_timur = id2.filter(ee.Filter.eq('ADM1_NAME', 'Nusatenggara Timur'))\n",
        "sula_teng = id2.filter(ee.Filter.eq('ADM1_NAME', 'Sulawesi Tengah'))\n",
        "sula_ara = id2.filter(ee.Filter.eq('ADM1_NAME', 'Sulawesi Tenggara'))\n",
        "suma_barat = id2.filter(ee.Filter.eq('ADM1_NAME', 'Sumatera Barat'))\n",
        "suma_utara = id2.filter(ee.Filter.eq('ADM1_NAME', 'Sumatera Utara'))\n",
        "papua_barat = id2.filter(ee.Filter.eq('ADM1_NAME', 'Papua Barat'))\n",
        "papua = id2.filter(ee.Filter.eq('ADM1_NAME', 'Papua'))\n",
        "\n",
        "sea6 = maluku.merge(maluku_utara)\n",
        "\n",
        "\n",
        "sea7 = day.merge(dki_jakarta).merge(jambi).merge(jawa_tengah).merge(jawa_timur)                  \\\n",
        "            .merge(kali_barat)\n",
        "\n",
        "\n",
        "sea8 = riau.merge(sulawesi_barat).merge(sulawesi_selatan).merge(nad).merge(bali)    \\\n",
        "\n",
        "\n",
        "sea9 = jawa_barat.merge(sulawesi_utara).merge(sumatera_selatan).merge(kepulauan_riau)\n",
        "\n",
        "sea10 = bengkulu.merge(nusa_timur).merge(sula_teng).merge(sula_ara)\n",
        "\n",
        "sea11 = suma_barat\n",
        "\n",
        "sea12 = papua_barat\n",
        "\n",
        "sea13 = bangka_belitung.merge(banten).merge(gorontalo)\n",
        "\n",
        "sea14 = kali_sela.merge(kali_teng).merge(kali_timur).merge(lampung).merge(nusa_barat)\n",
        "\n",
        "sea15 = suma_utara\n",
        "\n",
        "sea16 = papua\n",
        "\n",
        "\n",
        "sea = sea1.merge(sea2).merge(sea3).merge(sea4).merge(sea5).merge(sea6) \\\n",
        "      .merge(sea7).merge(sea8).merge(sea9).merge(sea10) \\\n",
        "      .merge(sea11).merge(sea12).merge(sea13).merge(sea14)\n",
        "\n",
        "# SERVIR Central America\n",
        "sca1 = es.merge(gt).merge(ho)\n",
        "sca2 = pa;\n",
        "sca3 = bz.merge(cr)\n",
        "\n",
        "sca = sca1.merge(sca2).merge(sca3)\n",
        "\n",
        "servir = swa.merge(sca).merge(sea).merge(shkh).merge(samz)\n",
        "\n",
        "#sca_test = sca1.merge(sca2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrnyNsuYc69c"
      },
      "source": [
        "Visualize SERVIR countries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocwV5Y4fjRtS"
      },
      "outputs": [],
      "source": [
        "Map = geemap.Map(center = (0, 0), zoom = 2)\n",
        "Map.addLayer(servir, {}, 'SERVIR Countries')\n",
        "Map.addLayerControl()\n",
        "Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PvBOE-do4CO"
      },
      "source": [
        "# Part 2: Get Slope and Elevation Data\n",
        "\n",
        "Currently we are only getting elevation data.\n",
        "\n",
        "We will use elevation data from NASA's Shuttle Radar Topography Mission (SRTM) [Farr and Kobrick 2000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUmICAbwPg4Y"
      },
      "outputs": [],
      "source": [
        "#srtm = ee.Image(\"USGS/SRTMGL1_003\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XKtNfWdC1a8"
      },
      "outputs": [],
      "source": [
        "copernicus = ee.ImageCollection(\"COPERNICUS/DEM/GLO30\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzUfNMVTp8JM"
      },
      "outputs": [],
      "source": [
        "# Make a function that\n",
        "\n",
        "def histfunc(aoi):\n",
        "  #srtm_clip = srtm.clip(aoi)\n",
        "  #srtm_mod = ee.Algorithms.Terrain(srtm_clip).select(['elevation', 'slope'])\n",
        "  cop_clip = copernicus.filterBounds(aoi).mosaic().clip(aoi).select(['DEM']).rename(['elevation'])\n",
        "  cop_mod = ee.Algorithms.Terrain(cop_clip)#.select(['DEM', 'slope'])\n",
        "\n",
        "  histdic = cop_mod.reduceRegion(**{\n",
        "      'reducer': ee.Reducer.histogram(None, 1),\n",
        "      'geometry': aoi.geometry(),\n",
        "      'scale': 30,\n",
        "      'maxPixels': 1e19\n",
        "  })\n",
        "\n",
        "  dic = histdic.getInfo()\n",
        "\n",
        "  # Get Elevation Data\n",
        "  elvdic = dic.get('elevation')\n",
        "  #elvdic = dic.get('DEM')\n",
        "\n",
        "  minev = elvdic.get('bucketMin')\n",
        "  stepev = elvdic.get('bucketWidth')\n",
        "  elv_pixcounts = elvdic.get('histogram')\n",
        "\n",
        "  # Get Slope Data\n",
        "  slodic = dic.get('slope')\n",
        "\n",
        "  minslo = slodic.get('bucketMin')\n",
        "  stepslo = slodic.get('bucketWidth')\n",
        "  slo_pixcounts = slodic.get('histogram')\n",
        "\n",
        "  elvals = []   # Elevation Values\n",
        "  elvpc = []    # Elevation Pixel Counts\n",
        "\n",
        "  slovals = []   # Slope Values\n",
        "  slopc = []     # Slope Pixel Counts\n",
        "\n",
        "  for i in range(len(elv_pixcounts)):\n",
        "    elv_count = stepev * i\n",
        "    ev = minev\n",
        "    ev += elv_count\n",
        "    elvals.append(ev)\n",
        "    elvpc.append(elv_pixcounts[i])\n",
        "\n",
        "  for j in range(len(slo_pixcounts)):\n",
        "    slo_count = stepslo * j\n",
        "    slo = minslo\n",
        "    slo += slo_count\n",
        "    slovals.append(slo)\n",
        "    slopc.append(slo_pixcounts[j])\n",
        "\n",
        "  return elvals, elvpc, slovals, slopc\n",
        "  #return dic.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvh-RyhNqfyB"
      },
      "source": [
        "# SERVIR West Africa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO8WNyQtqkho"
      },
      "source": [
        "SWA1 - 6 Ran in about 10 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6jFnDFIn3Kz"
      },
      "outputs": [],
      "source": [
        "# SERVIR West Afirca\n",
        "#swa1ev = histfunc(swa1)\n",
        "\n",
        "swa1ev, swa1ep, swa1sv, swa1sp = histfunc(swa1)        # swa1ev = SERVIR West Africa Region 1 Elevation Values\n",
        "                                                       # swa1ep = SERVIR West Africa Region 1 Elevation Pixel Counts\n",
        "                                                       # swa1sv = SERVIR West Africa Region 1 Slope Values\n",
        "                                                       # swa1sp = SERVIR West Africa Region 1 Slope Pixel Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TWYosJAx5TS"
      },
      "outputs": [],
      "source": [
        "swa2ev, swa2ep, swa2sv, swa2sp = histfunc(swa2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v42f8-ax_fy"
      },
      "outputs": [],
      "source": [
        "swa3ev, swa3ep, swa3sv, swa3sp = histfunc(swa3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciqZ4iAiyCLr"
      },
      "outputs": [],
      "source": [
        "swa4ev, swa4ep, swa4sv, swa4sp = histfunc(swa4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYpk7B97yFR0"
      },
      "outputs": [],
      "source": [
        "swa5ev, swa5ep, swa5sv, swa5sp = histfunc(swa5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65c_S2fJyJaR"
      },
      "outputs": [],
      "source": [
        "swa6ev, swa6ep, swa6sv, swa6sp = histfunc(swa6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw_vu8ExyLHb"
      },
      "outputs": [],
      "source": [
        "swa7ev, swa7ep, swa7sv, swa7sp = histfunc(swa7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBENyns7yPXg"
      },
      "outputs": [],
      "source": [
        "swa8ev, swa8ep, swa8sv, swa8sp = histfunc(swa8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdbqOjgQyQ4N"
      },
      "outputs": [],
      "source": [
        "swa9ev, swa9ep, swa9sv, swa9sp = histfunc(swa9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6tTaO6TySih"
      },
      "outputs": [],
      "source": [
        "swa10ev, swa10ep, swa10sv, swa10sp = histfunc(swa10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMr3YZ_1yT-O"
      },
      "outputs": [],
      "source": [
        "swa11ev, swa11ep, swa11sv, swa11sp = histfunc(swa11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_2uVP8nyV5g"
      },
      "outputs": [],
      "source": [
        "swa12ev, swa12ep, swa12sv, swa12sp = histfunc(swa12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK8kn_oeqeTD"
      },
      "source": [
        "# SERVIR HKH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5WuJrsnp7m_"
      },
      "outputs": [],
      "source": [
        "shkh1ev, shkh1ep, shkh1sv, shkh1sp = histfunc(shkh1)\n",
        "shkh2ev, shkh2ep, shkh2sv, shkh2sp = histfunc(shkh2)\n",
        "shkh3ev, shkh3ep, shkh3sv, shkh3sp = histfunc(shkh3)\n",
        "shkh4ev, shkh4ep, shkh4sv, shkh4sp = histfunc(shkh4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhwQtRaKqqXV"
      },
      "source": [
        "# SERVIR Southeast Asia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RLQj1tmqX2e"
      },
      "outputs": [],
      "source": [
        "sea1ev, sea1ep, sea1sv, sea1sp = histfunc(sea1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygESQbkMlcfO"
      },
      "outputs": [],
      "source": [
        "sea2ev, sea2ep, sea2sv, sea2sp = histfunc(sea2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lyc7HuQ0V03n"
      },
      "outputs": [],
      "source": [
        "sea3ev, sea3ep, sea3sv, sea3sp = histfunc(sea3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTldPlgkUVOx"
      },
      "outputs": [],
      "source": [
        "sea4ev, sea4ep, sea4sv, sea4sp = histfunc(sea4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtCEz0SrqPoC"
      },
      "outputs": [],
      "source": [
        "sea5ev, sea5ep, sea5sv, sea5sp = histfunc(sea5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lTZfha8laIr"
      },
      "outputs": [],
      "source": [
        "sea6ev, sea6ep, sea6sv, sea6sp = histfunc(sea6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD81xG3Tlkta"
      },
      "outputs": [],
      "source": [
        "sea7ev, sea7ep, sea7sv, sea7sp = histfunc(sea7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ9ejdskpbS1"
      },
      "outputs": [],
      "source": [
        "sea8ev, sea8ep, sea8sv, sea8sp = histfunc(sea8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb5fPBXuxbXo"
      },
      "outputs": [],
      "source": [
        "sea9ev, sea9ep, sea9sv, sea9sp = histfunc(sea9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndEjumSe6QSi"
      },
      "outputs": [],
      "source": [
        "sea10ev, sea10ep, sea10sv, sea10sp = histfunc(sea10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CGEQM_v7q34"
      },
      "outputs": [],
      "source": [
        "sea11ev, sea11ep, sea11sv, sea11sp = histfunc(sea11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1ztazyS9-mk"
      },
      "outputs": [],
      "source": [
        "sea12ev, sea12ep, sea12sv, sea12sp = histfunc(sea12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBQV2b5DjRiO"
      },
      "outputs": [],
      "source": [
        "sea13ev, sea13ep, sea13sv, sea13sp = histfunc(sea13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06ilJbU1jaP4"
      },
      "outputs": [],
      "source": [
        "sea14ev, sea14ep, sea14sv, sea14sp = histfunc(sea14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqjUJtnMsXax"
      },
      "outputs": [],
      "source": [
        "sea15ev, sea15ep, sea15sv, sea15sp = histfunc(sea15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daAHfxCst9R9"
      },
      "outputs": [],
      "source": [
        "sea16ev, sea16ep, sea16sv, sea16sp = histfunc(sea16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkR_Z9_7q7cp"
      },
      "source": [
        "# SERVIR Amazonia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMgyfrO8qPld"
      },
      "outputs": [],
      "source": [
        "samz1ev, samz1ep, samz1sv, samz1sp = histfunc(samz1)\n",
        "samz2ev, samz2ep, samz2sv, samz2sp = histfunc(samz2)\n",
        "samz3ev, samz3ep, samz3sv, samz3sp = histfunc(samz3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dB3FMePxCnp2"
      },
      "outputs": [],
      "source": [
        "samz4ev, samz4ep, samz4sv, samz4sp = histfunc(samz4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnxMDnhdWdir"
      },
      "outputs": [],
      "source": [
        "samz5ev, samz5ep, samz5sv, samz5sp = histfunc(samz5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdmjSZy7yNEa"
      },
      "outputs": [],
      "source": [
        "samz6ev, samz6ep, samz6sv, samz6sp = histfunc(samz6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ-niXT8yU7A"
      },
      "outputs": [],
      "source": [
        "samz7ev, samz7ep, samz7sv, samz7sp = histfunc(samz7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOratVXt8Jgo"
      },
      "outputs": [],
      "source": [
        "samz8ev, samz8ep, samz8sv, samz8sp = histfunc(samz8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJiFdp94ETEL"
      },
      "outputs": [],
      "source": [
        "samz9ev, samz9ep, samz9sv, samz9sp = histfunc(samz9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq7l-pVYEW4g"
      },
      "outputs": [],
      "source": [
        "samz10ev, samz10ep, samz10sv, samz10sp = histfunc(samz10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On5VKfBXKlpC"
      },
      "outputs": [],
      "source": [
        "samz11ev, samz11ep, samz11sv, samz11sp = histfunc(samz11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-TUdbnaKobZ"
      },
      "outputs": [],
      "source": [
        "samz12ev, samz12ep, samz12sv, samz12sp = histfunc(samz12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HQpTZqXN8lA"
      },
      "outputs": [],
      "source": [
        "samz13ev, samz13ep, samz13sv, samz13sp = histfunc(samz13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jtt6fJAKPkDl"
      },
      "outputs": [],
      "source": [
        "samz14ev, samz14ep, samz14sv, samz14sp = histfunc(samz14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3CexCugYXmV"
      },
      "outputs": [],
      "source": [
        "samz15ev, samz15ep, samz15sv, samz15sp = histfunc(samz15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lYqDcJyYbI3"
      },
      "outputs": [],
      "source": [
        "samz16ev, samz16ep, samz16sv, samz16sp = histfunc(samz16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WkNCPzxgv6R"
      },
      "outputs": [],
      "source": [
        "samz17ev, samz17ep, samz17sv, samz17sp = histfunc(samz17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naT0aFw_g1-N"
      },
      "outputs": [],
      "source": [
        "samz18ev, samz18ep, samz18sv, samz18sp = histfunc(samz18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OriS5bZloSM"
      },
      "outputs": [],
      "source": [
        "samz19ev, samz19ep, samz19sv, samz19sp = histfunc(samz19)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-dwFOf8n30M"
      },
      "outputs": [],
      "source": [
        "samz20ev, samz20ep, samz20sv, samz20sp = histfunc(samz20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q1FzL8bqHt7"
      },
      "outputs": [],
      "source": [
        "samz21ev, samz21ep, samz21sv, samz21sp = histfunc(samz21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQMbzTCg_wbc"
      },
      "outputs": [],
      "source": [
        "samz22ev, samz22ep, samz22sv, samz22sp = histfunc(samz22)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyPORegvFEXl"
      },
      "outputs": [],
      "source": [
        "samz23ev, samz23ep, samz23sv, samz23sp = histfunc(samz23)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAWCvKsmr5lg"
      },
      "source": [
        "# SERVIR Central America"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c91ouDJSqPhj"
      },
      "outputs": [],
      "source": [
        "sca1ev, sca1ep, sca1sv, sca1sp = histfunc(sca1)\n",
        "sca2ev, sca2ep, sca2sv, sca2sp = histfunc(sca2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZiPyUOZdziR"
      },
      "outputs": [],
      "source": [
        "sca3ev, sca3ep, sca3sv, sca3sp = histfunc(sca3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4HJPYo1sTNS"
      },
      "source": [
        "Now we have to aggregate the elevations and pixel counts into two arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApTYLl6Csjqa"
      },
      "outputs": [],
      "source": [
        "#from os import sched_getaffinity\n",
        "elevation_arrays = [sca1ev, sca2ev, sca3ev,                            # SERVIR Central America\n",
        "                    sea1ev, sea2ev, sea3ev, sea4ev, sea5ev, sea6ev,    # SERVIR Southeast Asia\n",
        "                    sea7ev, sea8ev, sea9ev, sea10ev, sea11ev, sea12ev,\n",
        "                    swa1ev, swa2ev, swa3ev, swa4ev, swa5ev, swa6ev,    # SERVIR West Africa\n",
        "                    swa7ev, swa8ev, swa9ev, swa10ev, swa11ev, swa12ev,\n",
        "                    shkh1ev, shkh2ev, shkh3ev, shkh4ev,                # SERVIR Hindu Kush Himalaya\n",
        "                    samz1ev, samz2ev, samz3ev, samz4ev,                # SERVIR Amazonia\n",
        "                    samz5ev, samz6ev, samz7ev, samz8ev,\n",
        "                    samz9ev, samz10ev, samz11ev, samz12ev,\n",
        "                    samz13ev, samz14ev, samz15ev,\n",
        "                    samz18ev, samz19ev, samz20ev, samz21ev,\n",
        "                    samz22ev, samz23ev]# samz16ev, samz17ev]           # SAMZ16 and SAMZ17 no longer working\n",
        "\n",
        "# Pixel Count Arrays\n",
        "elv_pc_arrays = [sca1ep, sca2ep, sca3ep,                            # SERVIR Central America\n",
        "             sea1ep, sea2ep, sea3ep, sea4ep, sea5ep, sea6ep,        # SERVIR Southeast Asia\n",
        "             sea7ep, sea8ep, sea9ep, sea10ep, sea11ep, sea12ep,\n",
        "             swa1ep, swa2ep, swa3ep, swa4ep, swa5ep, swa6ep,        # SERVIR West Africa\n",
        "             swa7ep, swa8ep, swa9ep, swa10ep, swa11ep, swa12ep,\n",
        "             shkh1ep, shkh2ep, shkh3ep, shkh4ep,                    # SERVIR Hinduu Kush Himalaya\n",
        "             samz1ep, samz2ep, samz3ep, samz4ep,                    # SERVIR Amazonia\n",
        "             samz5ep, samz6ep, samz7ep, samz8ep,\n",
        "             samz9ep, samz10ep, samz11ep, samz12ep, samz13ep,\n",
        "             samz14ep, samz15ep, samz18ep,\n",
        "             samz19ep, samz20ep, samz21ep, samz22ep, samz23ep]        # samz16ep, samz17ep,\n",
        "\n",
        "# Slope value arrays\n",
        "slope_arrays = [sca1sv, sca2sv, sca3sv,                               # SERVIR Central America\n",
        "                sea1sv, sea2sv, sea3sv, sea4sv, sea5sv, sea6sv,       # SERVIR Southeast Asia\n",
        "                sea7sv, sea8sv, sea9sv, sea10sv, sea11sv, sea12sv,\n",
        "                swa1sv, swa2sv, swa3sv, swa4sv, swa5sv, swa6sv,       # SERVIR West Africa\n",
        "                swa7sv, swa8sv, swa9sv, swa10sv, swa11sv, swa12sv,\n",
        "                shkh1sv, shkh2sv, shkh3sv, shkh4sv,                   # SERVIR Hinduu Kush Himalaya\n",
        "                samz1sv, samz2sv, samz3sv, samz4sv, samz5sv,\n",
        "                samz6sv, samz7sv, samz8sv, samz9sv, samz10sv,         # SERVIR Amazonia\n",
        "                samz11sv, samz12sv, samz13sv, samz14sv, samz15sv,\n",
        "                samz18sv, samz19sv,\n",
        "                samz20sv, samz21sv, samz22sv, samz23sv]               # samz16sv, samz17sv,          # samz16ep, samz17ep,\n",
        "\n",
        "# slope pixel count arrays\n",
        "slo_pc_arrays = [sca1sp, sca2sp, sca3sp,                              # SERVIR Central America\n",
        "                sea1sp, sea2sp, sea3sp, sea4sp, sea5sp, sea6sp,      # SERVIR Southeast Asia\n",
        "                sea7sp, sea8sp, sea9sp, sea10sp, sea11sp, sea12sp,\n",
        "                swa1sp, swa2sp, swa3sp, swa4sp, swa5sp, swa6sp,      # SERVIR West Africa\n",
        "                swa7sp, swa8sp, swa9sp, swa10sp, swa11sp, swa12sp,\n",
        "                shkh1sp, shkh2sp, shkh3sp, shkh4sp,                  # SERVIR Hinduu Kush Himalaya\n",
        "                samz1sp, samz2sp, samz3sp, samz4sp, samz5sp,         # SERVIR Amazonia\n",
        "                samz6sp, samz7sp, samz8sp, samz9sp, samz10sp,\n",
        "                samz11sp, samz12sp, samz13sp, samz14sp, samz15sp,\n",
        "                samz18sp, samz19sp,\n",
        "                samz20sp, samz21sp, samz22sp, samz23sp]                # samz16sp, samz17sp,\n",
        "\n",
        "sanity_check = len(elevation_arrays) == len(elv_pc_arrays) == len(slope_arrays) == len(slo_pc_arrays)\n",
        "print(\"Are the arrays above equal in length? Should be true:\", sanity_check)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H63ilkgUofip"
      },
      "source": [
        "Now we will use a nested for-loop to aggregate all elevation values into an array called `el_master`, all elevation pixel count arrays into an array called `elv_pc_master`, all slope values into an array called `sl_master`, and all slope pixel count arrays into an array called `sl_pc_master`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7YkJbXqsYwO"
      },
      "outputs": [],
      "source": [
        "el_master = []\n",
        "elv_pc_master = [] # Elevation Pixel Count\n",
        "sl_master = []\n",
        "sl_pc_master = []\n",
        "\n",
        "\n",
        "for i in range(len(elevation_arrays)):     # For all elevation arrays\n",
        "  eva = elevation_arrays[i]              # Elevation Value Array\n",
        "  epca = elv_pc_arrays[i]                 # Elevation Pixel Count Array\n",
        "  sva = slope_arrays[i]                  # Slope Value Array\n",
        "  spca = slo_pc_arrays[i]                 # Slope Pixel Count Array\n",
        "  for j in range(len(eva)):\n",
        "    ev_val = eva[j]                       # Elevation value of interest\n",
        "    epc_val = epca[j]                     # Elevation Pixel Count value of\n",
        "\n",
        "\n",
        "    if ev_val not in el_master:           # If the elevation value is not in master elevation array...\n",
        "      if ev_val > -420 and ev_val < 8848 and epc_val != 0:         # Mask out bad data (elevation lower than lowest elevation and higher than highest elevation) and disregard elevations that have a pixel value of 0.\n",
        "        el_master.append(ev_val)            # Append the elevation value of interest into the master elevation array\n",
        "        elv_pc_master.append(epc_val)     # Append the pixel count value of interest into the master pixel count array\n",
        "    else:                                  # If elevation value is not in master elevation array...\n",
        "      indexo = el_master.index(ev_val)    # Get the index value of the elevation value in the master array\n",
        "      elv_pc_master[indexo] += epc_val    # Add the pixel value of interest to the existing pixel count in the master pixel count array\n",
        "\n",
        "\n",
        "  for k in range(len(sva)):\n",
        "    sv_val = sva[k]                       # Slope Value of interest\n",
        "    spc_val = spca[k]                     # Slope Pixel Count Value of interest\n",
        "    if sv_val not in sl_master:\n",
        "      sl_master.append(sv_val)\n",
        "      sl_pc_master.append(spc_val)\n",
        "    else:\n",
        "      indexi = sl_master.index(sv_val)\n",
        "      sl_pc_master[indexi] += spc_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh5vbKwyG2mM"
      },
      "source": [
        "# Part 4: Visualize Data and Find Quartile Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofEeDBx-G4zT"
      },
      "source": [
        "### Part 4 Section 1: Elevation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FTg5pCM7eQX"
      },
      "source": [
        "First, we have to sort the elevation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdmNi1NXG75O"
      },
      "outputs": [],
      "source": [
        "plt.plot(el_master, elv_pc_master, '*b')\n",
        "plt.ylabel('Elevation Pixel Count')\n",
        "plt.xlabel('Elevation [meters]')\n",
        "plt.title('Elevation distribution across SERVIR regions (COPERNICUS GLO-30 DEM) ')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54CFAAhpHoKV"
      },
      "outputs": [],
      "source": [
        "#Find total pixels\n",
        "sum = 0\n",
        "for p in range(len(el_master)):\n",
        "  elval = el_master[p]\n",
        "  elpixc = elv_pc_master[p]\n",
        "  sum += elpixc\n",
        "print('Total Number of Valid Pixels for Elevation Data: {0:2.2e}'.format(sum))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JcNdJMp7q7V"
      },
      "outputs": [],
      "source": [
        "num_pix = sum / 4\n",
        "num_pix\n",
        "\n",
        "print(\"We want to split the data into four equal groups, each containing approximately {0:2.2e} pixels\".format(num_pix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxieFqAWQzsE"
      },
      "source": [
        "Let's sort the elevation data and pixel counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUE7_3QuQ22O"
      },
      "outputs": [],
      "source": [
        "# Put the data into a 2D Array\n",
        "elv2d = [[]]\n",
        "for u in range(len(el_master)):\n",
        "  elval = el_master[u]\n",
        "  elpixc = elv_pc_master[u]\n",
        "\n",
        "  mylist = []\n",
        "  mylist.append(elval)\n",
        "  mylist.append(elpixc)\n",
        "\n",
        "  elv2d.append(mylist)\n",
        "# Sort the data by the first column\n",
        "elv2d_sorted = sorted(elv2d)[1:]\n",
        "\n",
        "# Put the data back into 2 1-D Arrays\n",
        "elval_sorted = []\n",
        "elpc_sorted = []\n",
        "\n",
        "for v in range(len(elv2d_sorted)):\n",
        "  elvaoi = elv2d_sorted[v]              # Elevation Array of interest\n",
        "  elval_sorted.append(elvaoi[0])\n",
        "  elpc_sorted.append(elvaoi[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn10on0KAzoV"
      },
      "source": [
        "Use a for loop to find the ideal values to split each group into four groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvSCqp2kA4Is"
      },
      "outputs": [],
      "source": [
        "q1sum = 0\n",
        "q2sum = 0\n",
        "q3sum = 0\n",
        "q4sum = 0\n",
        "q1_val = 0\n",
        "q2_val = 0\n",
        "q3_val = 0\n",
        "q4_val = 0\n",
        "\n",
        "# Quartile 1\n",
        "for s in range(len(elval_sorted)):           # len(elval_sorted\n",
        "  elval = elval_sorted[s]\n",
        "  elpixc = elpc_sorted[s]\n",
        "  q1diffold =np.abs(q1sum - num_pix)\n",
        "  q1sum += elpixc\n",
        "  q1diff = np.abs(q1sum - num_pix)\n",
        "  if q1diffold < q1diff:\n",
        "    q1_val = elval\n",
        "    break\n",
        "\n",
        "\n",
        "print(\"Number of pixels with an elevation greater than {0} and less than {1} meters:  {2:2.2e}\".format(-420, q1_val, q1sum))\n",
        "\n",
        "# Quartile 2\n",
        "for w in range(len(elval_sorted)):\n",
        "  elval = elval_sorted[w]\n",
        "  elpixc = elpc_sorted[w]\n",
        "  if elval > q1_val:\n",
        "    q2diffold = np.abs(q2sum - num_pix)\n",
        "    q2sum += elpixc\n",
        "    q2diff = np.abs(q2sum - num_pix)\n",
        "    if q2diffold < q2diff:\n",
        "      q2_val = elval\n",
        "      break\n",
        "\n",
        "print(\"Number of pixels with an elevation greater than {0} and less than {1} meters:   {2:2.2e}\".format(q1_val, q2_val, q2sum))\n",
        "\n",
        "# Quartile 3\n",
        "for x in range(len(elval_sorted)):\n",
        "  elval = elval_sorted[x]\n",
        "  elpixc = elpc_sorted[x]\n",
        "  if elval > q2_val:\n",
        "    q3diffold = np.abs(q3sum - num_pix)\n",
        "    q3sum += elpixc\n",
        "    q3diff = np.abs(q3sum - num_pix)\n",
        "    if q3diffold < q3diff:\n",
        "      q3_val = elval\n",
        "      break\n",
        "\n",
        "print(\"Number of pixels with an elevation greater than {0} and less than {1} meters:   {2:2.2e}\".format(q2_val, q3_val, q3sum))\n",
        "\n",
        "\n",
        "# Quartile 4\n",
        "for y in range(len(elval_sorted)):\n",
        "  elval = elval_sorted[y]\n",
        "  elpixc = elpc_sorted[y]\n",
        "  if elval > q3_val:\n",
        "    q4sum += elpixc\n",
        "\n",
        "\n",
        "print(\"Number of pixels with an elevation greater than {0} meters:                     {1:2.2e}\".format(q3_val, q4sum))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07nXRhwkbrvY"
      },
      "source": [
        "This splits the data up into four roughly equal groups. There is no way to evenly split the data. Below shows an attempt to manually find even quartiles, but there is still a difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMdaDxwgAzDS"
      },
      "outputs": [],
      "source": [
        "q1vm = 140\n",
        "q2vm = 300\n",
        "q3vm = 496\n",
        "sum1test = 0\n",
        "sum2test = 0\n",
        "sum3test = 0\n",
        "sum4test = 0\n",
        "\n",
        "for alpha in range(len(elval_sorted)):\n",
        "  elval = elval_sorted[alpha]\n",
        "  elpc = elpc_sorted[alpha]\n",
        "  if elval <= q1vm:\n",
        "    sum1test += elpc\n",
        "  if elval > q1vm and elval <= q2vm:\n",
        "    sum2test += elpc\n",
        "  if elval > q2vm and elval <= q3vm:\n",
        "    sum3test += elpc\n",
        "  if elval > q3vm:\n",
        "    sum4test += elpc\n",
        "\n",
        "print('Number of Pixels with an elevation greater than {0} but less than {1} meters: {2:2.2e}'.format(\"-420\", q1vm, sum1test))\n",
        "print('Number of Pixels with an elevation greater than {0} but less than {1} meters:  {2:2.2e}'.format(q1vm, q2vm, sum2test))\n",
        "print('Number of Pixels with an elevation greater than {0} but less than {1} meters:  {2:2.2e}'.format(q2vm, q3vm, sum3test))\n",
        "print('Number of Pixels with an elevation greater than {0} meters:                    {1:2.2e}'.format(q3vm, sum4test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU38KfDSHUp0"
      },
      "source": [
        "## Part 4 Section 2: Slope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvVZLAkcHBus"
      },
      "outputs": [],
      "source": [
        "plt.plot(sl_master, sl_pc_master, '-r')\n",
        "plt.grid()\n",
        "plt.title(\"Slope Distribution in SERVIR regions accoridng to Copernicus GLO-30 and Earth Engine's Terrain Algorithm\", fontsize = 8)\n",
        "plt.xlabel('Slope [degrees]')\n",
        "plt.ylabel('Pixel Count')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTG2tTpky-mb"
      },
      "outputs": [],
      "source": [
        "# Put the data into a 2D Array\n",
        "sl2d = [[]]\n",
        "for beta in range(len(sl_master)):\n",
        "  slval = sl_master[beta]\n",
        "  slpixc = sl_pc_master[beta]\n",
        "\n",
        "  mylist = []\n",
        "  mylist.append(slval)\n",
        "  mylist.append(slpixc)\n",
        "\n",
        "  sl2d.append(mylist)\n",
        "# Sort the data by the first column\n",
        "sl2d_sorted = sorted(sl2d)[1:]\n",
        "\n",
        "# Put the data back into 2 1-D Arrays\n",
        "slval_sorted = []\n",
        "slpc_sorted = []\n",
        "\n",
        "for gamma in range(len(sl2d_sorted)):\n",
        "  slaoi = sl2d_sorted[gamma]              # Slope Array of interest\n",
        "  slval_sorted.append(slaoi[0])\n",
        "  slpc_sorted.append(slaoi[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvLfUD0QIQF_"
      },
      "outputs": [],
      "source": [
        "#Find total pixels\n",
        "sum2 = 0\n",
        "for q in range(len(sl_master)):\n",
        "  slval = sl_master[q]\n",
        "  slpixc = sl_pc_master[q]\n",
        "  sum2 += slpixc\n",
        "\n",
        "slopepix = sum2 / 4\n",
        "\n",
        "print(\"Number of pixels each group should have: {0:e}\".format(slopepix))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDO7kIgHz9TP"
      },
      "outputs": [],
      "source": [
        "sq1sum = 0\n",
        "sq2sum = 0\n",
        "sq3sum = 0\n",
        "sq4sum = 0\n",
        "sq1_val = 0\n",
        "sq2_val = 0\n",
        "sq3_val = 0\n",
        "sq4_val = 0\n",
        "\n",
        "# Quartile 1\n",
        "for delta in range(len(slval_sorted)):\n",
        "  slval = slval_sorted[delta]\n",
        "  slpixc = slpc_sorted[delta]\n",
        "  sq1diffold =np.abs(sq1sum - slopepix)\n",
        "  sq1sum += slpixc\n",
        "  sq1diff = np.abs(sq1sum - slopepix)\n",
        "  if sq1diffold < sq1diff:\n",
        "    sq1_val += slval\n",
        "    break\n",
        "\n",
        "print('Number of Pixels with an elevation less than {0} meters:                     {1:2.2e}'.format(sq1_val, sq1sum))\n",
        "\n",
        "# Quartile 2\n",
        "for epsilon in range(len(slval_sorted)):\n",
        "  slval = slval_sorted[epsilon]\n",
        "  slpixc = slpc_sorted[epsilon]\n",
        "  if slval > sq1_val:\n",
        "    sq2diffold = np.abs(sq2sum - slopepix)\n",
        "    sq2sum += slpixc\n",
        "    sq2diff = np.abs(sq2sum - slopepix)\n",
        "    if sq2diffold < sq2diff:\n",
        "      sq2_val = slval\n",
        "      break\n",
        "\n",
        "print('Number of Pixels with an elevation greater than {0} but less than {1} meters:  {2:2.2e}'.format(sq1_val, sq2_val, sq2sum))\n",
        "\n",
        "# Quartile 3\n",
        "for zeta in range(len(slval_sorted)):\n",
        "  slval = slval_sorted[zeta]\n",
        "  slpixc = slpc_sorted[zeta]\n",
        "  if slval > sq2_val:\n",
        "    sq3diffold = np.abs(sq3sum - slopepix)\n",
        "    sq3sum += slpixc\n",
        "    sq3diff = np.abs(sq3sum - slopepix)\n",
        "    if sq3diffold < sq3diff:\n",
        "      sq3_val = slval\n",
        "      break\n",
        "\n",
        "print('Number of Pixels with an elevation greater than {0} but less than {1} meters: {2:2.2e}'.format(sq2_val, sq3_val, sq3sum))\n",
        "\n",
        "\n",
        "# Quartile 4\n",
        "for eta in range(len(slval_sorted)):\n",
        "  slval = slval_sorted[eta]\n",
        "  slpixc = slpc_sorted[eta]\n",
        "  if slval > sq3_val:\n",
        "    sq4sum += slpixc\n",
        "\n",
        "\n",
        "print('Number of Pixels with an elevation less than {0} meters:                    {1:2.2e}'.format(sq3_val, sq4sum))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm49UrBR2rSO"
      },
      "source": [
        "Now that we know the approximate values, let's manually alter them to more evenly distribute the points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhTfGcRX2Eyo"
      },
      "outputs": [],
      "source": [
        "sq1vm = 2\n",
        "sq2vm = 4\n",
        "sq3vm = 9\n",
        "ss1 = 0        # Slope Sum 1\n",
        "ss2 = 0\n",
        "ss3 = 0\n",
        "ss4 = 0\n",
        "\n",
        "for theta in range(len(slval_sorted)):\n",
        "  slval = slval_sorted[theta]\n",
        "  slpc = slpc_sorted[theta]\n",
        "  if slval <= sq1vm:\n",
        "    ss1 += slpc\n",
        "  if slval > sq1vm and slval <= sq2vm:\n",
        "    ss2 += slpc\n",
        "  if slval > sq2vm and slval <= sq3vm:\n",
        "    ss3 += slpc\n",
        "  if slval > sq3vm:\n",
        "    ss4 += slpc\n",
        "\n",
        "print('Number of Pixels with a slope less than {0} degrees:                    {1:e}'.format(sq1vm, ss1))\n",
        "print('Number of Pixels with a slope greater than {0} but less than {1} degrees: {2:e}'.format(sq1vm, sq2vm, ss2))\n",
        "print('Number of Pixels with a slope greater than {0} but less than {1} degrees: {2:e}'.format(sq2vm, sq3vm, ss3))\n",
        "print('Number of Pixels with a slope greater than {0} degrees:                 {1:e}'.format(sq3vm, ss4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOgwFy7kpGtX"
      },
      "source": [
        "# Part 5: References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1FCYoB6pauH"
      },
      "source": [
        "Farr, Tom G., and Mike Kobrick. \"Shuttle Radar Topography Mission produces a wealth of data.\" Eos, Transactions American Geophysical Union 81.48 (2000): 583-585."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}